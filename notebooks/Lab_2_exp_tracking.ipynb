{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "# Experiment Tracking and Model Registry Lab\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this lab you will each download a new dataset and attempt to train a good model, and use mlflow to keep track of all of your experiments, log your metrics, artifacts and models, and then register a final set of models for \"deployment\", though we won't actually deploy them anywhere yet.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Your goal is **not** to become a master at MLFlow - this is not a course on learning all of the ins and outs of MLFlow. Instead, your goal is to understand when and why it is important to track your model development process (tracking experiments, artifacts and models) and to get into the habit of doing so, and then learn at least the basics of how MLFlow helps you do this so that you can then compare with other tools that are available.\n",
    "\n",
    "## Data\n",
    "\n",
    "You can choose your own dataset to use here. It will be helpful to choose a dataset that is already fairly clean and easy to work with. You can even use a dataset that you've used in a previous course. We will do a lot of labs where we do different things with datasets, so if you can find one that is interesting enough for modeling, it should work for most of the rest of the course. \n",
    "\n",
    "There are tons of places where you can find open public datasets. Choose something that interests you, but don't overthink it.\n",
    "\n",
    "[Kaggle Datasets](https://www.kaggle.com/datasets)  \n",
    "[HuggingFace Datasets](https://huggingface.co/docs/datasets/index)  \n",
    "[Dagshub Datasets](https://dagshub.com/datasets/)  \n",
    "[UCI](https://archive.ics.uci.edu/ml/datasets.php)  \n",
    "[Open Data on AWS](https://registry.opendata.aws/)  \n",
    "[Yelp](https://www.yelp.com/dataset)  \n",
    "[MovieLens](https://grouplens.org/datasets/movielens/)  \n",
    "And so many more...\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Once you have selected a set of data, create a brand new experiment in MLFlow and begin exploring your data. Do some EDA, clean up, and learn about your data. You do not need to begin tracking anything yet, but you can if you want to (e.g. you can log different versions of your data as you clean it up and do any feature engineering). Do not spend a ton of time on this part. Your goal isn't really to build a great model, so don't spend hours on feature engineering and missing data imputation and things like that.\n",
    "\n",
    "Once your data is clean, begin training models and tracking your experiments. If you intend to use this same dataset for your final project, then start thinking about what your model might look like when you actually deploy it. For example, when you engineer new features, be sure to save the code that does this, as you will need this in the future. If your final model has 1000 complex features, you might have a difficult time deploying it later on. If your final model takes 15 minutes to train, or takes a long time to score a new batch of data, you may want to think about training a less complex model.\n",
    "\n",
    "Now, when tracking your experiments, at a *minimum*, you should:\n",
    "\n",
    "1. Try at least 3 different ML algorithms (e.g. linear regression, decision tree, random forest, etc.).\n",
    "2. Do hyperparameter tuning for **each** algorithm.\n",
    "3. Do some very basic feature selection, and repeat the above steps with these reduced sets of features.\n",
    "4. Identify the top 3 best models and note these down for later.\n",
    "6. Choose the **final** \"best\" model that you would deploy or use on future data, stage it (in MLFlow), and run it on the test set to get a final measure of performance. Don't forget to log the test set metric.\n",
    "7. Be sure you logged the exact training, validation, and testing datasets for the 3 best models, as well as hyperparameter values, and the values of your metrics.  \n",
    "8. Push your code to Github. No need to track the mlruns folder, the images folder, any datasets, or the sqlite database in git.\n",
    "\n",
    "### Turning It In\n",
    "\n",
    "In the MLFlow UI, next to the refresh button you should see three vertical dots. Click the dots and then download your experiments as a csv file. Open the csv file in Excel and highlight the rows for your top 3 models from step 4, highlight the run where you applied your best model to the test set, and then save as an excel file. Take a snapshot of the Models page in the MLFLow UI showing the model you staged in step 6 above. Submit the excel file and the snapshot to Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [3 different ML algorithms]()\n",
    "2. [Hyperparameter Tuning]()\n",
    "3. [Feature Selection]()\n",
    "4. [Identify Top 3 best model]()\n",
    "5. [Final Best model]()\n",
    "6. [Training Validation and Testing Models]()\n",
    "7. [Github]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command to Start `mlflow ui --backend-store-uri sqlite:///mlflow.db`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from joblib import dump, load\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.15.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/Users/drewhoang/Desktop/mlops-course/notebooks/mlruns/1', creation_time=1742579036555, experiment_id='1', last_update_time=1742579036555, lifecycle_stage='active', name='amazon-experiment', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create new experiment\n",
    "# experiment_name = \"Amazon_Polarity_Experiment\"\n",
    "# try:\n",
    "#     experiment_id = mlflow.create_experiment(experiment_name)\n",
    "# except:\n",
    "#     experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"amazon-experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Hugging Face...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from Hugging Face\n",
    "print(\"Loading dataset from Hugging Face...\")\n",
    "dataset = load_dataset(\"fancyzhx/amazon_polarity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (3600000, 3)\n",
      "Test dataset shape: (400000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train dataset shape: {train_df.shape}\")\n",
    "print(f\"Test dataset shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and content for feature extraction\n",
    "train_df['text'] = train_df['title'] + \" \" + train_df['content']\n",
    "test_df['text'] = test_df['title'] + \" \" + test_df['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    train_df['text'], \n",
    "    train_df['label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=train_df['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, min_df=5, max_df=0.7)\n",
    "X_train = tfidf_vectorizer.fit_transform(train_data)\n",
    "X_val = tfidf_vectorizer.transform(val_data)\n",
    "X_test = tfidf_vectorizer.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_labels\n",
    "y_val = val_labels\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class distribution:\n",
      "label\n",
      "1    1800000\n",
      "0    1800000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClass distribution:\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('processed_data', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(tfidf_vectorizer, 'processed_data/tfidf_vectorizer.joblib')\n",
    "pickle.dump((X_train, y_train), open('processed_data/train_data.pkl', 'wb'))\n",
    "pickle.dump((X_val, y_val), open('processed_data/val_data.pkl', 'wb'))\n",
    "pickle.dump((X_test, y_test), open('processed_data/test_data.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing completed and artifacts logged.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    mlflow.log_artifacts('processed_data', artifact_path='processed_data')\n",
    "    mlflow.set_tag(\"process\", \"preprocessing\")\n",
    "    mlflow.log_params({\n",
    "        \"vectorizer\": \"TF-IDF\",\n",
    "        \"max_features\": 5000,\n",
    "        \"min_df\": 5,\n",
    "        \"max_df\": 0.7\n",
    "    })\n",
    "mlflow.end_run()\n",
    "print(\"Data preprocessing completed and artifacts logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def feature_selection(model, X_train, y_train, X_val, y_val, n_features=1000):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    if hasattr(model, 'coef_'):\n",
    "        importances = np.abs(model.coef_[0])\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        return X_train, X_val\n",
    "    \n",
    "    feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "    sorted_idx = np.argsort(importances)[::-1][:n_features]\n",
    "    \n",
    "    # Get the selected feature names\n",
    "    selected_features = feature_names[sorted_idx]\n",
    "    \n",
    "    # Create a new vectorizer with only selected features\n",
    "    new_vectorizer = TfidfVectorizer(vocabulary=selected_features)\n",
    "    new_vectorizer.fit(train_data)\n",
    "    \n",
    "    X_train_selected = new_vectorizer.transform(train_data)\n",
    "    X_val_selected = new_vectorizer.transform(val_data)\n",
    "    \n",
    "    return X_train_selected, X_val_selected, new_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_selector = LogisticRegression(max_iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection completed and artifacts logged.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    # Top 1000 features\n",
    "    X_train_1000, X_val_1000, vectorizer_1000 = feature_selection(lr_selector, X_train, y_train, X_val, y_val, n_features=1000)\n",
    "    dump(vectorizer_1000, 'processed_data/tfidf_vectorizer_1000.joblib')\n",
    "    pickle.dump((X_train_1000, y_train), open('processed_data/train_data_1000.pkl', 'wb'))\n",
    "    pickle.dump((X_val_1000, y_val), open('processed_data/val_data_1000.pkl', 'wb'))\n",
    "    \n",
    "    # Top 500 features\n",
    "    X_train_500, X_val_500, vectorizer_500 = feature_selection(lr_selector, X_train, y_train, X_val, y_val, n_features=500)\n",
    "    dump(vectorizer_500, 'processed_data/tfidf_vectorizer_500.joblib')\n",
    "    pickle.dump((X_train_500, y_train), open('processed_data/train_data_500.pkl', 'wb'))\n",
    "    pickle.dump((X_val_500, y_val), open('processed_data/val_data_500.pkl', 'wb'))\n",
    "    \n",
    "    mlflow.log_artifacts('processed_data', artifact_path='processed_data')\n",
    "    mlflow.set_tag(\"process\", \"feature_selection\")\n",
    "    mlflow.log_params({\n",
    "        \"feature_selector\": \"LogisticRegression\",\n",
    "        \"feature_set_sizes\": \"5000, 1000, 500\"\n",
    "    })\n",
    "mlflow.end_run()\n",
    "print(\"Feature selection completed and artifacts logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_500, X_val_500, vectorizer_500 = feature_selection(lr_selector, X_train, y_train, X_val, y_val, n_features=500)\n",
    "dump(vectorizer_500, 'processed_data/tfidf_vectorizer_500.joblib')\n",
    "pickle.dump((X_train_500, y_train), open('processed_data/train_data_500.pkl', 'wb'))\n",
    "pickle.dump((X_val_500, y_val), open('processed_data/val_data_500.pkl', 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1: Logistic Regression with Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 500 features\n",
    "lr_params_500 = [\n",
    "    {\"C\": 0.1, \"solver\": \"liblinear\", \"max_iter\": 1000},\n",
    "    {\"C\": 1.0, \"solver\": \"liblinear\", \"max_iter\": 1000},\n",
    "    {\"C\": 10.0, \"solver\": \"liblinear\", \"max_iter\": 1000},\n",
    "]\n",
    "\n",
    "for i, params in enumerate(lr_params_500):\n",
    "    with mlflow.start_run():\n",
    "        # Create the model with parameters\n",
    "        lr = LogisticRegression(**params)\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.set_tags({\"Model\": \"LogisticRegression\", \"Feature Set\": \"500features\"})\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"feature_set\", \"500features\")\n",
    "        mlflow.log_param(\"feature_count\", X_train_500.shape[1])\n",
    "\n",
    "        # Train model\n",
    "        lr.fit(X_train_500, y_train)\n",
    "\n",
    "        # Evaluate on validation data\n",
    "        y_pred = lr.predict(X_val_500)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "        # Save model\n",
    "        model_path = f\"models/LogisticRegression_500features_{i+1}.pkl\"\n",
    "        with open(model_path, \"wb\") as f:\n",
    "            pickle.dump(lr, f)\n",
    "\n",
    "        # Create a sample input for model signature\n",
    "        input_example = X_train_500[:1]  # Just use the first training example\n",
    "\n",
    "        # Log model with signature\n",
    "        mlflow.sklearn.log_model(\n",
    "            lr, f\"LogisticRegression_500features\", input_example=input_example\n",
    "        )\n",
    "\n",
    "        mlflow.log_artifact(model_path)\n",
    "    \n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "lr_param_grid = {\n",
    "    \"C\": [0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "    \"solver\": [\"liblinear\", \"saga\"],\n",
    "    \"max_iter\": [1000],\n",
    "    \"class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "# Define feature set name\n",
    "feature_set_name = \"500features\"\n",
    "\n",
    "# Initialize storage for runs\n",
    "all_runs = []\n",
    "\n",
    "# Run with MLflow tracking\n",
    "with mlflow.start_run():\n",
    "    # Log basic parameters\n",
    "    mlflow.log_param(\"feature_set\", feature_set_name)\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"feature_count\", X_train.shape[1])\n",
    "    \n",
    "    # Log the full parameter grid\n",
    "    mlflow.log_param(\"param_grid\", str(lr_param_grid))\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    lr_grid = GridSearchCV(\n",
    "        LogisticRegression(random_state=42),\n",
    "        lr_param_grid,\n",
    "        cv=3,\n",
    "        scoring=\"f1\",\n",
    "        verbose=1,\n",
    "    )\n",
    "    lr_grid.fit(X_train, y_train)\n",
    "\n",
    "    # Get best model and parameters\n",
    "    best_lr = lr_grid.best_estimator_\n",
    "    best_params = lr_grid.best_params_\n",
    "    \n",
    "    # Log best parameters from grid search\n",
    "    mlflow.log_params(best_params)\n",
    "    mlflow.log_param(\"best_cv_score\", lr_grid.best_score_)\n",
    "    \n",
    "    # Set tags for easy filtering in MLflow UI\n",
    "    mlflow.set_tags({\"Model\": \"LogisticRegression\", \"Feature Set\": feature_set_name})\n",
    "    \n",
    "    # Get predictions on validation set\n",
    "    y_pred = best_lr.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred, average='binary')\n",
    "    recall = recall_score(y_val, y_pred, average='binary')\n",
    "    f1 = f1_score(y_val, y_pred, average='binary')\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    \n",
    "    # Save model as pickle file\n",
    "    model_path = f\"models/LogisticRegression_{feature_set_name}.pkl\"\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(best_lr, f)\n",
    "    \n",
    "    # Log model with MLflow\n",
    "    mlflow.sklearn.log_model(\n",
    "        best_lr, \n",
    "        f\"LogisticRegression_{feature_set_name}\", \n",
    "        input_example=X_train[:1]\n",
    "    )\n",
    "    \n",
    "    # Log the pickle file as an artifact\n",
    "    mlflow.log_artifact(model_path)\n",
    "    \n",
    "    # Track this run's information\n",
    "    all_runs.append(\n",
    "        {\n",
    "            \"run_id\": mlflow.active_run().info.run_id,\n",
    "            \"model\": \"LogisticRegression\",\n",
    "            \"feature_set\": feature_set_name,\n",
    "            \"params\": best_params,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1,\n",
    "        }\n",
    "    )\n",
    "mlflow.end_run()\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Best LogisticRegression model (feature set: {feature_set_name}):\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Validation metrics:\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 2: Random Forest with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rf_params_500 = [\n",
    "    {\"n_estimators\": 100, \"max_depth\": 10, \"random_state\": 42},\n",
    "    {\"n_estimators\": 200, \"max_depth\": 15, \"random_state\": 42},\n",
    "    {\"n_estimators\": 300, \"max_depth\": None, \"random_state\": 42},\n",
    "]\n",
    "\n",
    "\n",
    "for i, params in enumerate(rf_params_500):\n",
    "    with mlflow.start_run(run_name=f\"RandomForest_500_{i+1}\"):\n",
    "        # Create the model with parameters\n",
    "        rf = RandomForestClassifier(**params)\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.set_tags({\"Model\": \"RandomForest\", \"Feature Set\": \"500features\"})\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"feature_set\", \"500features\")\n",
    "        mlflow.log_param(\"feature_count\", X_train_500.shape[1])\n",
    "\n",
    "        # Train model\n",
    "        rf.fit(X_train_500, y_train)\n",
    "\n",
    "        # Evaluate on validation data\n",
    "        y_pred = rf.predict(X_val_500)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "        # Save model\n",
    "        model_path = f\"models/RandomForest_500features_{i+1}.pkl\"\n",
    "        with open(model_path, \"wb\") as f:\n",
    "            pickle.dump(rf, f)\n",
    "\n",
    "        # Log model with signature\n",
    "        mlflow.sklearn.log_model(\n",
    "            rf, f\"RandomForest_500features\", input_example=X_train_500[:1]\n",
    "        )\n",
    "\n",
    "        mlflow.log_artifact(model_path)\n",
    "        \n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 3: Decision Tree with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree with 500 features\n",
    "dt_params_500 = [\n",
    "    {\"max_depth\": 5, \"min_samples_split\": 2, \"random_state\": 42},\n",
    "    {\"max_depth\": 10, \"min_samples_split\": 5, \"random_state\": 42},\n",
    "    {\"max_depth\": 15, \"min_samples_split\": 10, \"random_state\": 42},\n",
    "]\n",
    "\n",
    "for i, params in enumerate(dt_params_500):\n",
    "    with mlflow.start_run():\n",
    "        # Create the model with parameters\n",
    "        dt = DecisionTreeClassifier(**params)\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.set_tags({\"Model\": \"DecisionTree\", \"Feature Set\": \"500features\"})\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"feature_set\", \"500features\")\n",
    "        mlflow.log_param(\"feature_count\", X_train_500.shape[1])\n",
    "\n",
    "        # Train model\n",
    "        dt.fit(X_train_500, y_train)\n",
    "\n",
    "        # Evaluate on validation data\n",
    "        y_pred = dt.predict(X_val_500)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "        # Save model\n",
    "        model_path = f\"models/DecisionTree_500features_{i+1}.pkl\"\n",
    "        with open(model_path, \"wb\") as f:\n",
    "            pickle.dump(dt, f)\n",
    "\n",
    "        # Log model with signature\n",
    "        mlflow.sklearn.log_model(\n",
    "            dt, f\"DecisionTree_500features\", input_example=X_train_500[:1]\n",
    "        )\n",
    "\n",
    "        mlflow.log_artifact(model_path)\n",
    "    mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_models(n=3, experiment_name=\"amazon-experiment\", metric=\"f1_score\"):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if experiment is None:\n",
    "        raise ValueError(f\"Experiment '{experiment_name}' not found.\")\n",
    "    \n",
    "    # Search all runs in the experiment\n",
    "    runs = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "    \n",
    "    # Sort by the specified metric\n",
    "    metric_col = f\"metrics.{metric}\"\n",
    "    if metric_col not in runs.columns:\n",
    "        raise ValueError(f\"Metric '{metric}' not found in run data.\")\n",
    "    \n",
    "    # Find the top n runs\n",
    "    top_runs = runs.sort_values(metric_col, ascending=False).head(n)\n",
    "    \n",
    "    # Prepare the results for display\n",
    "    results = []\n",
    "    for i, (idx, run) in enumerate(top_runs.iterrows()):\n",
    "        model_info = {\n",
    "            'rank': i + 1,\n",
    "            'run_id': run.run_id,\n",
    "            'model_type': run.get('tags.Model', 'Unknown'),\n",
    "            'feature_set': run.get('params.feature_set', 'Unknown'),\n",
    "            'metric_value': run[metric_col],\n",
    "            'accuracy': run.get('metrics.accuracy', 0),\n",
    "            'precision': run.get('metrics.precision', 0),\n",
    "            'recall': run.get('metrics.recall', 0),\n",
    "            'f1_score': run.get('metrics.f1_score', 0)\n",
    "        }\n",
    "        \n",
    "        # Extract hyperparameters\n",
    "        params = {k.replace('params.', ''): v for k, v in run.items() if k.startswith('params.')}\n",
    "        model_info['hyperparameters'] = params\n",
    "        \n",
    "        results.append(model_info)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_models = find_top_models(n=3, experiment_name=\"amazon-experiment\", metric=\"f1_score\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 models:\n",
      "050dcc296f1146cdb6a3714865040b96\n",
      "1. LogisticRegression with feature set '500features'\n",
      "   F1 Score: 0.8785, Accuracy: 0.8784\n",
      "   Key hyperparameters: random_state=None, max_depth=None, ...\n",
      "0ea4cf72cba7488183a269081da261c5\n",
      "2. LogisticRegression with feature set '500features'\n",
      "   F1 Score: 0.8783, Accuracy: 0.8782\n",
      "   Key hyperparameters: random_state=None, max_depth=None, ...\n",
      "b206a94ace464b9684bc937580247c2c\n",
      "3. LogisticRegression with feature set '500features'\n",
      "   F1 Score: 0.8783, Accuracy: 0.8782\n",
      "   Key hyperparameters: random_state=None, max_depth=None, ...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop 3 models:\")\n",
    "for i, (_, model) in enumerate(top_models.iterrows()):\n",
    "    print(f\"{model['run_id']}\")\n",
    "    print(f\"{i+1}. {model['model_type']} with feature set '{model['feature_set']}'\")\n",
    "    print(f\"   F1 Score: {model['f1_score']:.4f}, Accuracy: {model['accuracy']:.4f}\")\n",
    "    print(f\"   Key hyperparameters: \", end=\"\")\n",
    "    hyperparams = model['hyperparameters']\n",
    "    # Display a few key hyperparameters\n",
    "    for param in list(hyperparams.keys())[:3]:  # Show first 3 params\n",
    "        if param not in ['feature_set', 'feature_count']:\n",
    "            print(f\"{param}={hyperparams[param]}\", end=\", \")\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(model, X_train, y_train, X_val, X_test, y_val, \n",
    "                      n_features=1000, tfidf_vectorizer=None, \n",
    "                      train_data=None, val_data=None, test_data=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Check if tfidf_vectorizer is None\n",
    "    if tfidf_vectorizer is None:\n",
    "        print(\"Warning: tfidf_vectorizer is None. Cannot perform feature selection.\")\n",
    "        return X_train, X_val, X_test, None\n",
    "    \n",
    "    # Fit the model if it's not already fitted\n",
    "    if not hasattr(model, 'coef_') and not hasattr(model, 'feature_importances_'):\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting model: {e}\")\n",
    "            return X_train, X_val, X_test, tfidf_vectorizer\n",
    "    \n",
    "    # Get feature importances based on model type\n",
    "    if hasattr(model, 'coef_'):\n",
    "        importances = np.abs(model.coef_[0])\n",
    "    elif hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        print(\"Model doesn't have coef_ or feature_importances_ attribute. Returning original features.\")\n",
    "        return X_train, X_val, X_test, tfidf_vectorizer\n",
    "    \n",
    "    # Safety check for feature names method\n",
    "    try:\n",
    "        # For scikit-learn >= 1.0\n",
    "        feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            # For scikit-learn < 1.0\n",
    "            feature_names = np.array(tfidf_vectorizer.get_feature_names())\n",
    "        except AttributeError:\n",
    "            print(\"Could not get feature names from vectorizer. Returning original features.\")\n",
    "            return X_train, X_val, X_test, tfidf_vectorizer\n",
    "    \n",
    "    # Ensure we don't request more features than available\n",
    "    n_features = min(n_features, len(feature_names))\n",
    "    \n",
    "    # Sort features by importance\n",
    "    sorted_idx = np.argsort(importances)[::-1][:n_features]\n",
    "    \n",
    "    # Get the selected feature names\n",
    "    selected_features = feature_names[sorted_idx]\n",
    "    \n",
    "    # Check if we have the original text data\n",
    "    if train_data is None or val_data is None or test_data is None:\n",
    "        print(\"Original text data is required for creating a new vectorizer. Returning original features.\")\n",
    "        return X_train, X_val, X_test, tfidf_vectorizer\n",
    "    \n",
    "    # Create a new vectorizer with only selected features\n",
    "    new_vectorizer = TfidfVectorizer(vocabulary=dict(zip(selected_features, range(len(selected_features)))))\n",
    "    \n",
    "    try:\n",
    "        # Transform all datasets with the new vectorizer\n",
    "        X_train_selected = new_vectorizer.fit_transform(train_data)\n",
    "        X_val_selected = new_vectorizer.transform(val_data)\n",
    "        X_test_selected = new_vectorizer.transform(test_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error transforming data with new vectorizer: {e}\")\n",
    "        return X_train, X_val, X_test, tfidf_vectorizer\n",
    "    \n",
    "    return X_train_selected, X_val_selected, X_test_selected, new_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: tfidf_vectorizer is None. Cannot perform feature selection.\n"
     ]
    }
   ],
   "source": [
    "X_train_500, X_val_500, X_test_500, vectorizer_500 = feature_selection(lr_selector, X_train, y_train, X_val, X_test, y_val, n_features=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/21 12:11:16 INFO mlflow.tracking.fluent: Experiment with name 'best-model-experiment' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "        \"500features\": (X_train_500, X_val_500, X_test_500, y_train, y_val, y_test),\n",
    "}\n",
    "top_models_df = pd.DataFrame([\n",
    "        {\n",
    "            'rank': 1,\n",
    "            'run_id': '050dcc296f1146cdb6a3714865040b96',\n",
    "            'model_type': 'LogisticRegression',\n",
    "            'feature_set': '500features'\n",
    "        },\n",
    "        {\n",
    "            'rank': 2,\n",
    "            'run_id': '0ea4cf72cba7488183a269081da261c5',\n",
    "            'model_type': 'LogisticRegression',\n",
    "            'feature_set': '500features'\n",
    "        },\n",
    "        {\n",
    "            'rank': 3,\n",
    "            'run_id': 'sb206a94ace464b9684bc937580247c2c',\n",
    "            'model_type': 'LogisticRegression',\n",
    "            'feature_set': '500features'\n",
    "        }\n",
    "    ])\n",
    "mlflow.set_experiment('best-model-experiment')\n",
    "for _, model_info in top_models_df.iterrows():\n",
    "        run_id = model_info['run_id']\n",
    "        feature_set = model_info['feature_set']\n",
    "        model_type = model_info['model_type']\n",
    "        \n",
    "        # Get the datasets for this feature set\n",
    "        if feature_set not in datasets:\n",
    "            print(f\"Warning: No datasets found for feature set {feature_set}\")\n",
    "            continue\n",
    "            \n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = datasets[feature_set]\n",
    "        \n",
    "        # Create a new run linked to the original\n",
    "        with mlflow.start_run():\n",
    "            mlflow.set_tag(\"parent_run_id\", run_id)\n",
    "            mlflow.set_tag(\"content\", \"datasets\")\n",
    "            mlflow.set_tag(\"model_type\", model_type)\n",
    "            mlflow.set_tag(\"feature_set\", feature_set)\n",
    "            \n",
    "            # Log dataset shapes\n",
    "            mlflow.log_param(\"X_train_shape\", str(X_train.shape))\n",
    "            mlflow.log_param(\"X_val_shape\", str(X_val.shape))\n",
    "            mlflow.log_param(\"X_test_shape\", str(X_test.shape))\n",
    "            mlflow.log_param(\"y_train_shape\", str(y_train.shape))\n",
    "            mlflow.log_param(\"y_val_shape\", str(y_val.shape))\n",
    "            mlflow.log_param(\"y_test_shape\", str(y_test.shape))\n",
    "            \n",
    "            # Save datasets as pickle files for reproducibility\n",
    "            dataset_path = f\"datasets_{model_type}_{feature_set}.pkl\"\n",
    "            with open(dataset_path, \"wb\") as f:\n",
    "                pickle.dump({\n",
    "                    \"X_train_shape\": X_train.shape,\n",
    "                    \"X_val_shape\": X_val.shape,\n",
    "                    \"X_test_shape\": X_test.shape,\n",
    "                    \"y_train_shape\": y_train.shape,\n",
    "                    \"y_val_shape\": y_val.shape,\n",
    "                    \"y_test_shape\": y_test.shape,\n",
    "                    \"X_train_sample\": X_train[:5] if X_train.shape[0] >= 5 else X_train,\n",
    "                    \"X_val_sample\": X_val[:5] if X_val.shape[0] >= 5 else X_val,\n",
    "                    \"X_test_sample\": X_test[:5] if X_test.shape[0] >= 5 else X_test,\n",
    "                }, f)\n",
    "            mlflow.log_artifact(dataset_path)\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 129516152 stored elements and shape (2880000, 5000)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: tfidf_vectorizer is None. Cannot perform feature selection.\n",
      "Created new registered model: production-LogisticRegression-500features-b\n",
      "Registered model version: 1\n",
      "Model production-LogisticRegression-500features-b version 1 is now in Production stage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'production-LogisticRegression-500features-b' already exists. Creating a new version of this model...\n",
      "Created version '1' of model 'production-LogisticRegression-500features-b'.\n",
      "/var/folders/n_/3_x326jx1px01h5tp8dtcgzw0000gn/T/ipykernel_34421/4124877315.py:56: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Evaluation Results:\n",
      "Accuracy: 0.4951\n",
      "Precision: 0.4859\n",
      "Recall: 0.1694\n",
      "F1 Score: 0.2512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:15: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/envs/mlops/lib/python3.11/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "Registered model 'production-LogisticRegression-500features-b' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'production-LogisticRegression-500features-b'.\n",
      "2025/03/21 15:04:47 WARNING mlflow.models.model: Input example should be provided to infer model signature if the model signature is not provided when logging the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final model production-LogisticRegression-500features-b (version 1) is now in Production stage and ready for deployment\n",
      "All evaluation metrics and artifacts have been logged to MLflow\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Set the MLflow experiment\n",
    "mlflow.set_experiment('best-model-selection')\n",
    "\n",
    "datasets = {\n",
    "    \"500features\": (X_train_500, X_val_500, X_test_500, y_train, y_val, y_test),\n",
    "}\n",
    "\n",
    "\n",
    "best_model_info = {\n",
    "    'run_id': '050dcc296f1146cdb6a3714865040b96',  \n",
    "    'model_type': 'LogisticRegression',\n",
    "    'feature_set': '500features',\n",
    "    'hyperparameters': {\"C\": 10.0, \"solver\": \"liblinear\", \"max_iter\": 1000} \n",
    "}\n",
    "\n",
    "# Unpack the information\n",
    "best_run_id = best_model_info['run_id']\n",
    "model_type = best_model_info['model_type']\n",
    "feature_set = best_model_info['feature_set']\n",
    "\n",
    "# Unpack test data\n",
    "X_train_500, X_val_500, X_test_500, vectorizer_500 = feature_selection(lr_selector, X_train, y_train, X_val, X_test, y_val, n_features=500)\n",
    "\n",
    "# Create a client for model registry operations\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "# Set a name for the registered model - this will be our production model\n",
    "model_name = f\"production-{model_type}-{feature_set}-b\"\n",
    "\n",
    "# Load the model from the run\n",
    "run_path = f\"runs:/{best_run_id}/{model_type}_{feature_set}\"\n",
    "best_model = mlflow.sklearn.load_model(run_path)\n",
    "\n",
    "# Register the model (create if it doesn't exist)\n",
    "try:\n",
    "    client.create_registered_model(model_name)\n",
    "    print(f\"Created new registered model: {model_name}\")\n",
    "except mlflow.exceptions.RestException:\n",
    "    print(f\"Model {model_name} already exists, will create a new version.\")\n",
    "\n",
    "# Register a new version\n",
    "model_details = mlflow.register_model(\n",
    "    model_uri=run_path,\n",
    "    name=model_name\n",
    ")\n",
    "\n",
    "print(f\"Registered model version: {model_details.version}\")\n",
    "\n",
    "# Transition directly to Production (since this is our final best model)\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_details.version,\n",
    "    stage=\"Production\",\n",
    "    archive_existing_versions=True  # Archive any existing production versions\n",
    ")\n",
    "\n",
    "print(f\"Model {model_name} version {model_details.version} is now in Production stage\")\n",
    "\n",
    "# Create a new run to log final test results\n",
    "with mlflow.start_run():\n",
    "    # Set parent run ID for lineage tracking\n",
    "    mlflow.set_tag(\"parent_run_id\", best_run_id)\n",
    "    mlflow.set_tag(\"model_type\", model_type)\n",
    "    mlflow.set_tag(\"feature_set\", feature_set)\n",
    "    mlflow.set_tag(\"evaluation\", \"final_test\")\n",
    "    mlflow.set_tag(\"stage\", \"Production\")\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    if 'hyperparameters' in best_model_info and best_model_info['hyperparameters']:\n",
    "        for param_name, param_value in best_model_info['hyperparameters'].items():\n",
    "            if param_name not in ['feature_set', 'feature_count']:  # Avoid duplicates\n",
    "                mlflow.log_param(param_name, param_value)\n",
    "                \n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "    mlflow.log_param(\"model_version\", model_details.version)\n",
    "    \n",
    "    # Get predictions - both class labels and probabilities if available\n",
    "    y_pred = best_model.predict(X_test_500)\n",
    "\n",
    "    \n",
    "    # Calculate comprehensive test metrics\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_precision = precision_score(y_test, y_pred, average='binary')\n",
    "    test_recall = recall_score(y_test, y_pred, average='binary')\n",
    "    test_f1 = f1_score(y_test, y_pred, average='binary')\n",
    "    \n",
    "    \n",
    "    # Log all test metrics\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"test_precision\", test_precision)\n",
    "    mlflow.log_metric(\"test_recall\", test_recall)\n",
    "    mlflow.log_metric(\"test_f1_score\", test_f1)\n",
    "    \n",
    "    # Log test data for reproducibility\n",
    "    test_data_path = f\"final_test_data_{model_type}_{feature_set}.pkl\"\n",
    "    with open(test_data_path, \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"X_test\": X_test_500, \n",
    "            \"y_test\": y_test,\n",
    "            \"test_metrics\": {\n",
    "                \"accuracy\": test_accuracy,\n",
    "                \"precision\": test_precision,\n",
    "                \"recall\": test_recall,\n",
    "                \"f1\": test_f1\n",
    "            }\n",
    "        }, f)\n",
    "    mlflow.log_artifact(test_data_path)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nFinal Test Evaluation Results:\")\n",
    "    print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Precision: {test_precision:.4f}\")\n",
    "    print(f\"Recall: {test_recall:.4f}\")\n",
    "    print(f\"F1 Score: {test_f1:.4f}\")\n",
    "\n",
    "    # Log the final model for deployment\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=best_model,\n",
    "        artifact_path=\"final_model\",\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "\n",
    "    # Return test metrics summary\n",
    "    test_metrics = {\n",
    "        \"accuracy\": test_accuracy,\n",
    "        \"precision\": test_precision,\n",
    "        \"recall\": test_recall,\n",
    "        \"f1\": test_f1\n",
    "    }\n",
    "\n",
    "# End the run\n",
    "mlflow.end_run()\n",
    "\n",
    "print(f\"\\nFinal model {model_name} (version {model_details.version}) is now in Production stage and ready for deployment\")\n",
    "print(f\"All evaluation metrics and artifacts have been logged to MLflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 22283 stored elements and shape (500, 5000)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model (rank 1)\n",
    "best_model_info = top_models_df.iloc[0]\n",
    "best_run_id = best_model_info['run_id']\n",
    "model_type = best_model_info['model_type']\n",
    "feature_set = best_model_info['feature_set']\n",
    "    \n",
    "# Unpack test data\n",
    "#X_train, X_val, X_test, y_train, y_val, y_test = datasets[feature_set]\n",
    "    \n",
    "# Create a client for model registry operations\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "    \n",
    "# Set a name for the registered model\n",
    "model_name = f\"best-{model_type}-{feature_set}\"\n",
    "    \n",
    "# Load the model from the run\n",
    "run_path = f\"runs:/{best_run_id}/{model_type}_{feature_set}\"\n",
    "best_model = mlflow.sklearn.load_model(run_path)\n",
    "\n",
    "# Register the model (create if it doesn't exist)\n",
    "try:\n",
    "    client.create_registered_model(model_name)\n",
    "except mlflow.exceptions.RestException:\n",
    "    print(f\"Model {model_name} already exists, will create a new version.\")\n",
    "    \n",
    "# Register a new version\n",
    "model_details = mlflow.register_model(\n",
    "    model_uri=run_path,\n",
    "    name=model_name\n",
    ")\n",
    "\n",
    "# Transition to staging\n",
    "client.transition_model_version_stage(\n",
    "    name=model_name,\n",
    "    version=model_details.version,\n",
    "    stage=\"Staging\"\n",
    ")\n",
    "\n",
    "# Create a new run to log test results\n",
    "with mlflow.start_run():\n",
    "    # Set parent run ID for lineage tracking\n",
    "    mlflow.set_tag(\"parent_run_id\", best_run_id)\n",
    "    mlflow.set_tag(\"model_type\", model_type)\n",
    "    mlflow.set_tag(\"feature_set\", feature_set)\n",
    "    mlflow.set_tag(\"evaluation\", \"test\")\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    for param_name, param_value in best_model_info['hyperparameters'].items():\n",
    "        if param_name not in ['feature_set', 'feature_count']:  # Avoid duplicates\n",
    "            mlflow.log_param(param_name, param_value)\n",
    "            \n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "    mlflow.log_param(\"model_version\", model_details.version)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Calculate test metrics\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_precision = precision_score(y_test, y_pred, average='binary')\n",
    "    test_recall = recall_score(y_test, y_pred, average='binary')\n",
    "    test_f1 = f1_score(y_test, y_pred, average='binary')\n",
    "    \n",
    "    # Log test metrics\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"test_precision\", test_precision)\n",
    "    mlflow.log_metric(\"test_recall\", test_recall)\n",
    "    mlflow.log_metric(\"test_f1_score\", test_f1)\n",
    "            \n",
    "    # Log test data for reproducibility\n",
    "    test_data_path = f\"test_data_{model_type}_{feature_set}.pkl\"\n",
    "    with open(test_data_path, \"wb\") as f:\n",
    "        pickle.dump({\"X_test_shape\": X_test.shape, \"y_test_shape\": y_test.shape}, f)\n",
    "    mlflow.log_artifact(test_data_path)\n",
    "    \n",
    "    # Return test metrics\n",
    "    test_metrics = {\n",
    "        \"accuracy\": test_accuracy,\n",
    "        \"precision\": test_precision,\n",
    "        \"recall\": test_recall,\n",
    "        \"f1\": test_f1\n",
    "    }\n",
    "\n",
    "mlflow.end_run()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_best_model(top_models_df, test_data):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Get the best model (rank 1)\n",
    "    best_model_info = top_models_df.iloc[0]\n",
    "    best_run_id = best_model_info['run_id']\n",
    "    model_type = best_model_info['model_type']\n",
    "    feature_set = best_model_info['feature_set']\n",
    "    \n",
    "    # Unpack test data\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    # Create a client for model registry operations\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    \n",
    "    # Set a name for the registered model\n",
    "    model_name = f\"best-{model_type}-{feature_set}\"\n",
    "    \n",
    "    # Load the model from the run\n",
    "    run_path = f\"runs:/{best_run_id}/{model_type}_{feature_set}\"\n",
    "    best_model = mlflow.sklearn.load_model(run_path)\n",
    "    \n",
    "    # Register the model (create if it doesn't exist)\n",
    "    try:\n",
    "        client.create_registered_model(model_name)\n",
    "    except mlflow.exceptions.RestException:\n",
    "        print(f\"Model {model_name} already exists, will create a new version.\")\n",
    "        \n",
    "    # Register a new version\n",
    "    model_details = mlflow.register_model(\n",
    "        model_uri=run_path,\n",
    "        name=model_name\n",
    "    )\n",
    "    \n",
    "    # Transition to staging\n",
    "    client.transition_model_version_stage(\n",
    "        name=model_name,\n",
    "        version=model_details.version,\n",
    "        stage=\"Staging\"\n",
    "    )\n",
    "    \n",
    "    # Create a new run to log test results\n",
    "    with mlflow.start_run():\n",
    "        # Set parent run ID for lineage tracking\n",
    "        mlflow.set_tag(\"parent_run_id\", best_run_id)\n",
    "        mlflow.set_tag(\"model_type\", model_type)\n",
    "        mlflow.set_tag(\"feature_set\", feature_set)\n",
    "        mlflow.set_tag(\"evaluation\", \"test\")\n",
    "        \n",
    "        # Log hyperparameters\n",
    "        for param_name, param_value in best_model_info['hyperparameters'].items():\n",
    "            if param_name not in ['feature_set', 'feature_count']:  # Avoid duplicates\n",
    "                mlflow.log_param(param_name, param_value)\n",
    "                \n",
    "        mlflow.log_param(\"model_name\", model_name)\n",
    "        mlflow.log_param(\"model_version\", model_details.version)\n",
    "        \n",
    "        # Predict on test data\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        # Calculate test metrics\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        test_precision = precision_score(y_test, y_pred, average='binary')\n",
    "        test_recall = recall_score(y_test, y_pred, average='binary')\n",
    "        test_f1 = f1_score(y_test, y_pred, average='binary')\n",
    "        \n",
    "        # Log test metrics\n",
    "        mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "        mlflow.log_metric(\"test_precision\", test_precision)\n",
    "        mlflow.log_metric(\"test_recall\", test_recall)\n",
    "        mlflow.log_metric(\"test_f1_score\", test_f1)\n",
    "                \n",
    "        # Log test data for reproducibility\n",
    "        test_data_path = f\"test_data_{model_type}_{feature_set}.pkl\"\n",
    "        with open(test_data_path, \"wb\") as f:\n",
    "            pickle.dump({\"X_test_shape\": X_test.shape, \"y_test_shape\": y_test.shape}, f)\n",
    "        mlflow.log_artifact(test_data_path)\n",
    "        \n",
    "        # Return test metrics\n",
    "        test_metrics = {\n",
    "            \"accuracy\": test_accuracy,\n",
    "            \"precision\": test_precision,\n",
    "            \"recall\": test_recall,\n",
    "            \"f1\": test_f1\n",
    "        }\n",
    "    mlflow.end_run()\n",
    "    return best_model, test_metrics, model_name, model_details.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'500features'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_models = pd.DataFrame([{\n",
    "        'rank': 1,\n",
    "        'run_id': 'sample_run_id',\n",
    "        'model_type': 'LogisticRegression',\n",
    "        'feature_set': '500features',\n",
    "        'hyperparameters': {'C': 1.0, 'solver': 'liblinear'}\n",
    "    }])\n",
    "    \n",
    "# Get test data for the best model's feature set\n",
    "best_feature_set = top_models.iloc[0]['feature_set']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStaging best model and evaluating on test set...\")\n",
    "best_feature_set = top_models.iloc[0]['feature_set']\n",
    "test_data = (datasets[best_feature_set][2], datasets[best_feature_set][5])  # X_test, y_test\n",
    "    \n",
    "best_model, test_metrics, model_name, model_version = stage_best_model(top_models, test_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
